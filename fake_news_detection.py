# -*- coding: utf-8 -*-
"""fake News Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vy6yfHZUqhb1MG7NFAyllKeu9iNXWBHp

load data sets

# New Section
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/

#loading the data
fake=pd.read_csv("/content/Fake.csv")
true=pd.read_csv("/content/True.csv")

#Creating a category for whether fake or not
#where 1 stand for fake news and 0 stands for true news

fake["category"]=0
true["category"]=1

#joining the data the two data frame and reseting index
df=pd.concat([fake,true]).reset_index(drop=True)

"""data cleaning"""

#checking the missing values in each columns
df.isna().sum()*100/len(df)
# we dont have any missing value

#checking if there is empty string in TEXT column
blanks=[]

#index,label and review of the doc
for index,text in df["text"].items(): # it will iter through index,label and review
    if text.isspace(): # if there is a space
        blanks.append(index) #it will be noted down in empty list

len(blanks)

# merging title with text

df["text"] =df["title"]+df["text"]

#we only need two columns rest can be ignored

df=df[["text","category"]]

import re
def clean_text(text):
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
    text = text.lower()  # Convert to lowercase
    return text
df['text'] = df['text'].apply(clean_text)

# install transformer library
!pip install transformers
!pip install datasets

from transformers import BertTokenizer, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
from datasets import Dataset
import torch



dataset = Dataset.from_pandas(df)
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# Tokenize the dataset
tokenized_datasets = dataset.map(tokenize_function,batched=True)

#Label Encoding
tokenized_datasets = tokenized_datasets.rename_column("category", "labels")

print("\nTokenized Dataset Example:")
print(tokenized_datasets)

# Load pre-trained BERT model
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1,
    weight_decay=0.01,
    save_total_limit=2,
    logging_dir="./logs",
    logging_steps=10,
    report_to = "none"
)

print(model)

train_dataset = tokenized_datasets.shuffle(seed=123).select(range(5000))
test_dataset = tokenized_datasets.shuffle(seed=123).select(range(5000))

# for checking f1 score , precision/recall
 !pip install scikit-learn

#Add a compute_metrics Function This function calculates metrics using predictions and labels.
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)  # Convert logits to class predictions
    return {
        "f1": f1_score(labels, predictions, average="weighted"),  # Use "binary" for binary classification
        "precision": precision_score(labels, predictions, average="weighted"),
        "recall": recall_score(labels, predictions, average="weighted"),
        "accuracy": accuracy_score(labels, predictions),
    }

# define tranier
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

# Evaluate the model
eval_results = trainer.evaluate()
print("Evaluation Results:", eval_results)

#Testing on New Data

def predict_News(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding="max_length", max_length=512)
    # model = model.to("cuda")
    inputs = inputs.to("cuda")
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    return {"real": predictions[0][1].item(), "fake": predictions[0][0].item()}

example_text = "Donald trump is the president of united states"
print("News Prediction:", predict_News(example_text))

import requests

def fact_check(query):
    api_key = "xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}&key={api_key}"
    response = requests.get(url)
    results = response.json()

    if "claims" in results:
        for claim in results["claims"]:
            if claim["claimReview"][0]["textualRating"] == "False":
                return claim["claimReview"][0]["textualRating"], claim["claimReview"][0]["title"]
    return None, None

from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer_t5 = T5Tokenizer.from_pretrained("t5-small")
model_t5 = T5ForConditionalGeneration.from_pretrained("t5-small")

def generate_correct_news(fact_text):
    input_text = f"correct this news: {fact_text}"
    input_ids = tokenizer_t5.encode(input_text, return_tensors="pt", max_length=512, truncation=True)
    outputs = model_t5.generate(input_ids, max_length=256)
    return tokenizer_t5.decode(outputs[0], skip_special_tokens=True)

"""def pipeline(input_news):
    # Step 1: Classify using BERT
    inputs = tokenizer(input_news, return_tensors="pt", truncation=True, max_length=256)
    inputs = inputs.to("cuda")
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)  # 0 = fake, 1 = real

    # Get the probability for the "fake" class (assuming index 0 is fake)
    fake_prob = predictions[0][0].item()  # Convert to Python float

    if fake_prob > 0.5:  # Changed to > 0.5 since 0 = fake in your comment
        # Step 2: Fact-Check API
        rating, correct_claim = fact_check(input_news)
        if correct_claim:
            # Step 3: Generate Correction
            generated_news = generate_correct_news(correct_claim)
            return f"NO. The news is wrong. {generated_news}"
        else:
            return "Claim marked as fake but no correction found in database."
    else:
        return "News is verified as real.""""

print(pipeline("Donald trump is the  President of us."))

def fact_check(query):
    api_key = "YOUR_API_KEY"  # Replace with your actual API key
    url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search?query={query}&key={api_key}"

    try:
        response = requests.get(url)
        response.raise_for_status()
        results = response.json()

        if "claims" in results and results["claims"]:
            # Get the most relevant claim
            most_relevant_claim = results["claims"][0]
            claim_text = most_relevant_claim.get("text", query)
            claim_review = most_relevant_claim.get("claimReview", [{}])[0]

            rating = claim_review.get("textualRating", "No rating available")
            publisher = claim_review.get("publisher", {}).get("name", "Unknown publisher")
            review_title = claim_review.get("title", "No title available")
            url = claim_review.get("url", "")

            return rating, {
                "original_query": query,
                "claim": claim_text,
                "rating": rating,
                "publisher": publisher,
                "title": review_title,
                "url": url
            }

        return None, {"message": "No fact-check results found"}

    except requests.exceptions.RequestException as e:
        return None, {"error": f"API request failed: {str(e)}"}

def generate_correct_news(fact_check_data):
    """Generate a corrected news statement based on fact-check data"""
    claim = fact_check_data.get("claim", "")
    rating = fact_check_data.get("rating", "")
    publisher = fact_check_data.get("publisher", "")
    url = fact_check_data.get("url", "")

    return f"According to {publisher}, the correct information is: '{claim}' (Rating: {rating}). Source: {url}"

def pipeline(input_news):
    # Step 1: Classify using BERT
    inputs = tokenizer(input_news, return_tensors="pt", truncation=True, max_length=256)
    inputs = inputs.to("cuda")
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)

    # Get the probability for the "fake" class (assuming index 0 is fake)
    fake_prob = predictions[0][0].item()

    if fake_prob > 0.5:
        # Step 2: Fact-Check API
        rating, fact_check_data = fact_check(input_news)

        if rating and isinstance(fact_check_data, dict):
            # Step 3: Generate Correction
            generated_news = ask_llama(fact_check_data)
            return f"NO. The news appears to be inaccurate. {generated_news}"
        else:
            generated_news = ask_llama(input_news)
            return f"The news was flagged as potentially unreliable. {generated_news}"
    else:
        return "The news appears to be credible based on our analysis."

print(pipeline("Narender modi is president of america"))

# Step 1: Install Ollama (ignore warnings)
!curl -fsSL https://ollama.com/install.sh | OLLAMA_SKIP_SYSTEMD=true sh

# Step 2: Start the Ollama server in background
!nohup ollama serve > /tmp/ollama.log 2>&1 &
!sleep 5  # Wait for server to start

# Step 3: Verify server is running
!curl http://localhost:11434/api/tags  # Should return empty {} if working

# Step 4: Pull Llama 3 (8B version - best for Colab)
!ollama pull llama3

# Step 5: Run inference
def ask_llama(prompt):
    import requests
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "llama3",
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()["response"]

# Test it
print(ask_llama("Narender modi is president of america "))

!pip install nbformat --upgrade  # Ensure latest version
import nbformat

# Load the notebook
notebook_path = "fake News Detection.ipynb"  # Replace with your file
notebook = nbformat.read(notebook_path, as_version=4)

# Remove 'widgets' metadata if it exists
if 'metadata' in notebook and 'widgets' in notebook['metadata']:
    del notebook['metadata']['widgets']

# Save the cleaned notebook
nbformat.write(notebook, "cleaned_notebook.ipynb")

